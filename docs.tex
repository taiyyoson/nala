\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{placeins}


%title 
\title{Nala Technical Documentation \& Handoff Details}
\author{Jade Custer, Alina Scantlebury, Taiyo Williamson}

\begin{document}
\maketitle

\tableofcontents
\newpage

%introduction to project 
\section{Introduction}
Nala is an AI Chatbot powered by Anthropic's Claude Sonnet 4.5. It is based off of real interactions between health coaches and participates from the Examen Tu Salud program created by Professor Kelly L'Engle and her students. Despite the overwhelming positive of the program, there was a gap. Having health coaches requires a lot of time and energy for the training of the coach as well as the meetings. To bridge the gap Kelly partnered with Professor Alark Joshi to create a chatbot powered by AI to bridge the gap. Together they sponsored the project outlined below.


%logins
\section{Logins}
\subsection{Email}
\begin{verbatim}
    email: chatbot.nala@gmail.com
    password: chatbot
\end{verbatim}

\subsection{Overleaf}
\begin{verbatim}
    email: chatbot.nala@gmail.com
    password: chatbot
\end{verbatim}

\subsection{Render}
\begin{verbatim}
    Use the login with gmail option. Then login with chatbot.nala@gmail.com
\end{verbatim}

\subsection{Firebase}
\begin{verbatim}
    email: chatbot.nala@gmail.com
    password: chatbot
\end{verbatim}

\subsection{Claude}
Claude logins are held by Professor Alark Joshi as he manages funds on the account.

\subsection{OpenAI}
OpenAI logins are held by Professor Alark Joshi as he manages funds on the account.



%important links
\section{Important Links}
\subsection{NALA Google Drive Folder}
\url{https://drive.google.com/drive/folders/1AlVHSyAl9hjNY-_Lub_O4M4lRmPFUuIV?usp=sharing}
\subsection{IAG Grant}
\url{https://drive.google.com/drive/folders/1pdNvT1_FSr-B0ULYQigfGv6uOxZOgQZj?usp=sharing}
\subsection{Github}
\url{https://github.com/taiyyoson/nala}


%imports
\section{Imports}
\subsection{env}
\begin{verbatim}
#API Keys
OPENAI_API_KEY=sk-proj-AriNqJQGcCeSSDrLg02tnvTH_kggDh_z_q8DqYm7gLh-
nYwrbjgF_M1KiMd8oEx30NIznr7wZYT3BlbkFJo9jtDDiOdbfOYbQjDjMTgyaaP7zEhC4nByr
QjHwCT4s6Z_nHqDgn-YkIJjOZC9Iu2xVm6mjcIA
ANTHROPIC_API_KEY=sk-ant-api03-
Xx9rFbZ97QftIP4ggyqJ0LmglbtH7Y6poczY4QXHb4sJr8IgXFsxSykmHFd2htYhsq-
U3QueqxBHzUy0qr7t9w-Zu8hswAA

#these were left in for ease of use for testing with the transcripts db locally
#database configs
# DB_HOST=localhost
# DB_NAME=chatbot_db
# DB_USER=postgres
# DB_PASSWORD=nala

#render database configs
DATABASE_URL=postgresql://chatbot_a6jr_user:0phfsdr7FeZbwJsSyXiLBDAnzVrSs
oaM@dpg-d3k4ugjuibrs73f12dsg-a.oregon-postgres.render.com/chatbot_a6jr

#conversation history db
CONVERSATION_DATABASE_URL=postgresql://nala_conversations_user:rBEJAdH0rE
3wy99ZWQr02Uff4kyw8m3u@dpg-d4eg5smmcj7s73cmadc0-a.oregon-
postgres.render.com/nala_conversations
\end{verbatim}



\subsection{Needed Importants for Local Testing}
\subsubsection{AI-backend specific}
\begin{enumerate}
    \item Installing PostgreSQL on Mac, Windows, or Linux
    \item Installing pgvector extension
    \item pip install --upgrade pip
    \item pip install pandas openai psycopg2-binary python-dotenv
    \item Upgrade to Python 3.11.5
    \item pip install openai==0.28
\end{enumerate}

%databases
\section{Databases \& Cloud}
\subsection{Transcript Database}
Hosted in Render on the Basic Tier. 

Database Schema:
\begin{verbatim}
    CREATE EXTENSION IF NOT EXISTS vector;

CREATE TABLE coaching_conversations (
    id SERIAL PRIMARY KEY,
    participant_response TEXT NOT NULL,
    coach_response TEXT NOT NULL,
    context_category VARCHAR(50),
    goal_type VARCHAR(50),
    confidence_level INTEGER,
    keywords TEXT,
    source_file TEXT,
    -- Store embedding of the participant_response for matching user queries
    participant_embedding vector(1536),
    -- Optional: embed the coach response for similarity search
    coach_embedding vector(1536),
    created_at TIMESTAMP DEFAULT NOW()
);

-- Index for fast similarity search
CREATE INDEX ON coaching_conversations
    USING ivfflat (participant_embedding vector_cosine_ops);
CREATE INDEX ON coaching_conversations (context_category);
CREATE INDEX ON coaching_conversations (goal_type);
\end{verbatim}

\subsection{History Database}
Hosted in Render on the Free tier. Needs to be upgraded to the Basic tier.

\subsection{Backend Server}
Hosted on Render on Free tier which causes spindown which causes the backend to be slow on wakeup.

\section{How to Run in Expo}
\subsection{Needed Importants for Local Testing}
\subsubsection{Frontend (Expo) Specific}
\begin{enumerate}
    \item Install Node.js (v18+ recommended)
    \item Install Expo CLI globally:
          \texttt{npm install -g expo-cli}
    \item Install all project dependencies:
          \texttt{npm install}
    \item Install Firebase dependencies:
    \begin{itemize}
        \item \texttt{npm install firebase}
        \item \texttt{npm install @react-native-async-storage/async-storage}
    \end{itemize}
    \item Ensure a testing environment is available:
    \begin{itemize}
        \item iOS Simulator via Xcode (macOS only), or
        \item Android Emulator via Android Studio, or
        \item Expo Go app on a physical device
    \end{itemize}
    \item Create a \texttt{.env} file with:
          \texttt{EXPO\_PUBLIC\_API\_URL}
    \item Restart Expo whenever environment variables change
    \item Start the development server:
          \texttt{npx expo start}
    \item Clear Expo cache if issues arise:
          \texttt{npx expo start -c}
\end{enumerate}

\subsection{Clearing Cache}
\begin{itemize}
    \item Use the following command to clear the Expo bundler cache:
    \begin{verbatim}
        npx expo start -c
    \end{verbatim}
\end{itemize}

\subsection{Common Issues}
\begin{itemize}
    \item \textbf{App stuck on splash screen:} Restart Expo, clear cache, or reinstall \texttt{node\_modules}.
    \item \textbf{Bundler not connecting:} Close all existing Expo/Metro windows and run \texttt{npx expo start} again.
    \item \textbf{"Network request failed":} The backend server is offline or the backend URL is incorrect.
\end{itemize}

%deployment
\section{Deployment}
The Nala frontend is a TypeScript-based React Native application developed using the Expo managed workflow. Deployment is handled through \texttt{Expo Publish}, which allows the team to distribute updates instantly without generating native iOS or Android binaries. This workflow provides a permanent hosted URL that can be opened through the Expo Go application on any device.

\subsection{Overview}

Because the project is built in TypeScript, Expo automatically transpiles all \texttt{.ts} and \texttt{.tsx} files into optimized JavaScript during the publish step. No native compilation is required, and updates are pushed to users immediately. This makes Expo Publish the most efficient deployment method for development, testing, and sponsor demonstrations.

\subsection{Publishing the Application}

Deployment is performed from the \texttt{frontend/} directory. First, ensure the developer is logged into Expo:

\begin{verbatim}
npx expo login
\end{verbatim}

Once authenticated, the project can be published using:

\begin{verbatim}
npx expo publish
\end{verbatim}

During this process, Expo:

\begin{itemize}
    \item transpiles TypeScript into production-ready JavaScript,
    \item bundles all screens, assets, and components,
    \item uploads the bundle to Expo's hosting service,
    \item generates a permanent public URL for the application.
\end{itemize}

\subsection{Permanent Access Link}

After a successful deployment, Expo outputs a permanent link of the form:

\begin{verbatim}
https://nala-chatbot.expo.app/
\end{verbatim}

This URL is persistent and always points to the latest published version of Nala. Anyone with the Expo Go application can open this link directly on iOS or Android.

\subsection{User Access Methods}

Users may open the deployed application in two ways:

\begin{itemize}
    \item \textbf{Direct Link:} Link will be given for web app.

\end{itemize}

\subsection{Updating the Deployment}

To deploy an updated version of Nala, simply run:

\begin{verbatim}
npx expo publish
\end{verbatim}

The permanent link remains the same, and users automatically receive the latest version upon opening the app.

\subsection{When a Full Build is Required}

A full EAS (Expo Application Services) build is only necessary when:

\begin{itemize}
    \item new native modules are added,
    \item platform-specific permissions change,
    \item a standalone \texttt{.ipa} or \texttt{.apk} is required,
\end{itemize}

For the purposes of development, classroom demonstration, and sponsor testing, Expo Publish is the recommended and most efficient deployment workflow. In order to not pay the apple developer fee before we test more thoroughly through the web app considering we are not using notifications. 

%transcripts from examen 
\section{Transcripts}
\subsection{Transcripting Formatting}
\begin{verbatim}
The transcripts need to follow this exact format:
HH:MM:SS Speaker: Text content here
HH:MM:SS Speaker: Text content here

Required Format Specifications:

1. Timestamp Format
Must be HH:MM:SS (e.g., 17:57:47, 12:14:05)
Two digits for hours, minutes, and seconds
Separated by colons

2. Speaker Labels
Must be exactly Coach: or Participant:
Capital C and capital P
Followed by a colon and space
Case-sensitive (the code looks for these exact strings)

3. Line Structure
17:57:47 Coach: Hello. How are you today.
17:57:52 Participant: I'm doing well, thank you.

4. What the Code Expects
The regex pattern is:
python
r'(\d{2}:\d{2}:\d{2})\s+(Coach|Participant):\s*(.*)'
This means:
(\d{2}:\d{2}:\d{2}) - Exactly 2 digits, colon, 2 digits, colon, 2 digits
\s+ - One or more spaces
(Coach|Participant) - Exactly "Coach" or "Participant"
: - A colon
\s* - Zero or more spaces
(.*) - The rest of the text

5. Conversation Pairing Logic
The code pairs conversations as:
Participant speaks first → stored temporarily
Coach responds → creates a pair with the participant's previous statement
Both are saved together as one row in the CSV

6. File Requirements
Files must be .txt format
UTF-8 encoding
Each line should be one complete turn (timestamp + speaker + content)
Blank lines are ignored

Example Valid Transcript:
17:15:00 Participant: I've been struggling with drinking enough water.
17:15:08 Coach: On a scale of 1 to 10, how confident are you
    that you can increase your water intake?
17:15:34 Participant: Maybe a seven. I have a water bottle
    but I forget to use it.
17:15:45 Coach: What reminders could help you remember
    to drink water?

What Won't Work:
-  5:15 PM Coach: Hello (wrong time format) 
-  17:15:00 coach: Hello (lowercase 'coach') 
-  17:15:00Coach: Hello (missing space) 
-  17:15:00 Health Coach: Hello (wrong speaker label)


\end{verbatim}




\subsection{Transcript Formatting Prompt}
\begin{verbatim}
Core Methodology for Transcript Fixing

1. Speaker Identification
Analyzed conversational patterns to distinguish coach vs participant
Used contextual clues: coaches ask questions, set agendas, provide
    guidance; participants share personal details, respond to prompts
Identified role-specific language patterns
    (e.g., "On a scale of 1-10," "SMART goals" = coach)

2. Consecutive Speaker Consolidation
Combined all sequential lines from the same speaker into single blocks
Maintained chronological flow while eliminating choppy
    back-and-forth formatting
Preserved natural conversation rhythm by keeping logical pause points

3. Transcription Error Correction
Fixed obvious speech-to-text errors
    (e.g., "ticket thing" → "takeout thing")
Corrected fragmented words and unclear phrases
Maintained original meaning while improving clarity

4. Sentence Structure Optimization
Combined sentence fragments into complete thoughts
Added proper punctuation for readability
Removed excessive filler words that didn't add meaning
Kept meaningful hesitations and natural speech patterns

5. Ellipses Reduction
Replaced "..." with appropriate alternatives:
Complete sentences where thoughts finished
Commas for brief pauses
Natural transition phrases
Only kept ellipses for genuine trailing off

What I Preserved:
Conversational Authenticity:
Natural speech patterns ("you know," "like," "I mean")
Emotional expressions and hesitations
Cultural references and personal context
Genuine uncertainty and thinking processes

Technical Accuracy:
Medical terminology and conditions
Program-specific language (SMART goals, confidence scales)
Proper names and specific references
Academic and professional contexts

Relationship Dynamics:
Coaching rapport and professional boundaries
Power dynamics between coach and participant
Cultural sensitivity in health coaching
Personal disclosure comfort levels

Quality Control Standards
Readability: Each exchange flows naturally while maintaining
    authentic voice
Parsability: Format works perfectly with automated scripts
Context Preservation: All coaching methodology and participant
    responses intact
Cultural Sensitivity: Maintained references to Latino/Latinx
    identity and experiences

Validation Approach
For each transcript, I:
Verified speaker assignments made logical sense
Ensured no meaning was lost in consolidation
Checked that coaching techniques remained clear
Confirmed participant authenticity was preserved
\end{verbatim}

\subsection{Transcripting Pipeline}
Starting off with raw transcripts for zoom specific steps have to be taken to ensure that the transcripts are ready for processing 
Zoom transcripts often miss out on words, or they are not the correct word 

\href{https://docs.google.com/document/d/1dY0QrQXPfry5mX7-XsT61Z0Np06krylbrOt1mUC-Bt8/edit?usp=sharing}{This is the Transcript Formatting Guide}

Transcript Formatting
Two options for getting it formatted:
\begin{enumerate}
\item Someone runs through and fixes the transcript by hand paying attention to formatting (see above)

\item Run it through AI then check through for mistakes. After doing some testing using Anthropic’s Claude Sonnet with this prompt does  a pretty good job of fixing the transcripts

Some issues you might run into:

\begin{enumerate}
\item Wrong speaker - ask Claude to fix the speaker 
if you notice at a certain point it mixes up the speaker copy and paste the mixed up part and tell Claude that the speaker is wrong. 
 \item Same speaker two lines in a row - Combine the lines 
\item Odd speaking/ Incomprehensible - Make a best judgment
\item Two speakers in the same speaker section - A back and forth within the same line needs to be separated out. Make a best guess on the timestamp. Be sure to follow the guidelines.
\end{enumerate}
\end{enumerate}

\begin{verbatim}
After all transcripts are in the transcript folder located within the 
github repository (located in AI-backend) then the chatbot_pipeline.py 
program can be ran which processes the transcripts in the transcript 
folder -> csv is created -> database is setup -> load embeddings using 
the csv -> put embeddings in database -> verify database is correct -> 
check database correctness with query 
\end{verbatim}

%AI backend code
\section{AI Backend}
The Nala AI-Backend is Python and the main AI being used is Claude Sonnet 4.5 with support for other versions of Claude or OpenAi. This part of the backend manages the session states, session managers, and the transcript pipeline.

\subsection{Folder Structure}
\begin{verbatim}
AI-backend/
    transcripts/
        Edited EXAMEN044_HCS1 - Erika Zuniga Sandoval.txt
        Other 57 transcripts here
        .
        .
        .    
    utils/
        constants.py
        database.py
        discovery_helpers.py
        goal_detection.py
        goal_parser.py
        init.py
        name_extraction.py
        program_loader.py
        smart_evaluation.py
        state_helpers.py
        state_prompts.py
        unified_storage.py    
    base_session_chatbot.py
    chatbot_pipeline.py
    coach_responses.csv
    constraintChecker.py
    load_embeddings.py
    parse_transcripts2.py
    programinfo.txt
    query.py
    rag_dynamic.py
    session1_manager.py
    session1.py
    session2_manager.py
    session2.py
    session3_manager.py
    session3.py
    session4_manager.py
    session4.py
    setup_database.py
    test_connection.py
    test_conversationdb.py
    test_session_persistence.py
\end{verbatim}

\subsection{Transcripts}
The transcripts folder holds the 58 edited zoom transcripts from the 2020 Examen Tu Salud sessions. It was processed through Claude first and then reexamined for issues. The transcripts were only reexamined by one person so there is a chance certain conversation pairs are not correct. 

\subsubsection{Transcript Database Injection}
The code that parses the transcripts and preps them to be injected into the database are located in the main AI-Backend folder.

\begin{itemize}
    \item \textbf{parsetranscripts2.py} - parses the transcript folder and each file to get conversation pairs, categorize the responses, get the confidence level, and extract keywords to set up for injection in the database
    \item \textbf{setup\_database.py} - resets databases and creates conversation tables as well as the index tables for searching
    \item \textbf{load\_embeddings.py} - prepares text for embedding and inserts the embeddings into the transcript database
    \item \textbf{query.py} - has a variety of search options including vector searching and context searching
    \item \textbf{coach\_responses.csv} - csv that holds conversation pairs, context type, goal categories, confidence level, keywords, and source file
    \item \textbf{chatbot\_pipeline.py} - runs parsetranscripts2, setup\_database, load\_embeddings, and query to inject the transcripts into the database in render and check that the database has been loaded
\end{itemize}
    
\subsection{Sessions}
The session context for Claude is built off of similar conversation pairs from the given transcripts as well as history from previous sessions and previous parts of the relevant conversation.

\textbf{rag\_dynamic.py} - This rag pipeline takes in context from the most similar conversations from the transcripts and gives it to claude to build a response with context of how it should speak and act like as a health coach. This is used as the basis of the sessions.

\subsubsection{Session 1}
\begin{itemize}
    \item \textbf{session1.py} - manages the session states (moving into the next state, choosing paths, etc.) for session 1
    \item \textbf{session1\_manager.py} - manages the interactive chat for use in terminal for debugging for session 1
\end{itemize}

\subsubsection{Session 2 \& 3}

\begin{itemize}
    \item \textbf{session2.py} - manages the session states (moving into the next state, choosing paths, etc.) for session 2
    \item \textbf{session2\_manager.py} - manages the interactive chat for use in terminal for debugging for session 2
    \item \textbf{session3.py} - manages the session states (moving into the next state, choosing paths, etc.) for session 3
    \item \textbf{session3\_manager.py} - manages the interactive chat for use in terminal for debugging for session 3
\end{itemize}

\subsubsection{Session 4}
\begin{itemize}
    \item \textbf{session4.py} - manages the session states (moving into the next state, choosing paths, etc.) for session 4
    \item \textbf{session4\_manager.py} - manages the interactive chat for use in terminal for debugging for session 4
\end{itemize}

\subsection{Util Folder}
The util folder holds utilities that were used in the sessions and were taken out for reusability.

\begin{itemize}
    \item \textbf{constants.py} - holds shared constants across session managers
    \item \textbf{database.py} - loads and saves session data as well as users in database
    \item \textbf{discovery\_helpers.py} - helper function to ask discovery question and check them off in session 1
    \item \textbf{goal\_detection.py} - helper function to detect goals
    \item \textbf{goal\_parser.py} - parses the goal to check if it meets requirements and can rebuild it to be more robust and action oriented
    \item \textbf{init.py} - utility to import utilities for session managers
    \item \textbf{name\_extraction.py} - helper function to extract name from text
    \item \textbf{program\_loader.py} - helper function to load in program information
    \item \textbf{smart\_evaluation.py} - helper functions evaluate if a goal is smart using an LLM as well as a heuristic version of the same function
    \item \textbf{state\_helpers.py} - helper utility to create state result and extracts useful information
    \item \textbf{state\_prompts.py} - holds prompts for each of the states for each session
    \item \textbf{unified\_storage.py} - utility to save and load sessions uniformly
\end{itemize} 

\subsection{Test Files}
The included test files are to check if the connection to the databases is stable as well as the load and save sessions are working as planned.

\begin{itemize}
    \item \textbf{test\_connection.py} - tests connection to the transcript database
    \item \textbf{test\_conversationdb.py} - tests connection to the conversation history database
    \item \textbf{test\_session\_persistence.py} - tests saving and loading sessions for history persistence
\end{itemize}


Important to have the tester files for connection because there was an issue on occasion where USFConnect would refuse to let us connect to port 5432 on Render's server for the transcript database which caused NALA to be down while this happens. 

\subsection{Extra files}
These are some extra files that are used in the sessions but need to be more easily accessible for changes by a non-technical team. \\

base\_session\_chatbot.py - base class for session\_manager files  \\

constraintChecker.py - helper functions that identify impossible promises and fixes them  \\

programinfo.txt  - txt file that houses the detailed program information for use in session 1 //

%backend code
\section{Backend}

The backend is a FastAPI application hosted on Render that serves as the bridge between the React Native frontend and the AI-backend session management system. It handles REST API routing, database operations, session state management, and conversation persistence.

\subsection{Architecture Overview}
The backend follows a service-oriented architecture with clear separation of concerns:

\begin{itemize}
    \item \textbf{Framework:} FastAPI (Python async web framework)
    \item \textbf{Hosting:} Render (Free tier with auto-spindown)
    \item \textbf{Database:} PostgreSQL with pgvector extension for embeddings
    \item \textbf{AI Integration:} Bridges to AI-backend folder containing RAG system and session managers
    \item \textbf{CORS:} Configured for React Native/Expo frontend
\end{itemize}

\subsection{Folder Structure}
\begin{verbatim}
backend/
├── app.py                  # FastAPI application and startup
├── config/
│   ├── settings.py         # Environment configuration
│   ├── database.py         # SQLAlchemy database setup
│   └── firebase_config.py  # Firebase admin initialization
├── routes/
│   ├── chat.py            # Chat and conversation endpoints
│   ├── session.py         # Session progress and data endpoints
│   ├── user.py            # User status and onboarding
│   └── health.py          # Health check endpoint
├── services/
│   ├── ai_service.py      # AI/RAG integration layer
│   ├── conversation_service.py  # Conversation management
│   └── database_service.py      # Database operations
├── models/
│   ├── conversation.py    # Conversation and message models
│   ├── session_progress.py # Session unlock/completion tracking
│   └── user.py            # User model
├── adapters/
│   ├── request_adapter.py  # Request transformation
│   └── response_adapter.py # Response formatting
├── tests/
│   ├── test_api_endpoints.py
│   └── test_database.py
└── archive/
    └── pubsub/            # Partial pub-sub implementation
\end{verbatim}

\subsection{Core Components}

\subsubsection{AIService (\texttt{services/ai\_service.py})}

Manages integration with the AI-backend RAG system. Initializes the appropriate session manager (Session 1-4) based on \texttt{session\_number} parameter, loads previous session data for continuity, and routes messages to the RAG chatbot.

Key responsibilities:
\begin{itemize}
    \item Session manager initialization (SessionBasedRAGChatbot, Session2RAGChatbot, etc.)
    \item Loading previous session data from database for Sessions 2-4
    \item Response generation via \texttt{generate\_response()} method
    \item Model selection (Claude Sonnet 4.5 or GPT-4o-mini)
\end{itemize}

\subsubsection{ConversationService (\texttt{services/conversation\_service.py})}

Handles conversation and message persistence in PostgreSQL. Creates or retrieves conversations, saves user and assistant messages with metadata, and maintains conversation history with timestamps.

\subsubsection{Session Routes (\texttt{routes/session.py})}

Manages session progress tracking and unlock logic:
\begin{itemize}
    \item \texttt{mark\_session\_complete()} - Marks session complete and unlocks next session after 7-day delay
    \item \texttt{get\_session\_data()} - Loads saved session data (goals, user profile) from database
    \item \texttt{get\_latest\_session()} - Retrieves most recent session for user
    \item \texttt{get\_user\_progress()} - Returns all session progress records for user
\end{itemize}

\subsubsection{Chat Routes (\texttt{routes/chat.py})}

Primary message handling endpoint at \texttt{POST /api/v1/chat/message}. Maintains AI service cache to preserve session state across messages within same conversation. Detects session completion via \texttt{end\_session} state and triggers session save. Automatically loads previous session data for Sessions 2-4.

\subsection{Database Models}

\subsubsection{Conversation \& Message}

Stores conversation metadata and individual messages with role (user/assistant), content, timestamp, and optional metadata (model used, RAG sources).

\subsubsection{SessionProgress}

Tracks completion status and unlock timing for each of the 4 sessions per user. Contains fields: \texttt{user\_id}, \texttt{session\_number}, \texttt{completed\_at}, \texttt{unlocked\_at}.

\subsection{Integration with AI-Backend}

The backend imports session managers directly from the \texttt{AI-backend/} folder. Session state is managed by the AI-backend session managers (session1.py through session4.py). When a session reaches \texttt{END\_SESSION} state, the backend calls \texttt{save\_session()} which persists user profile, goals, and chat history to the database via \texttt{utils/database.py} functions.

\subsection{Deployment}

Hosted on Render (Free tier). Free tier has auto-spindown after 15 minutes of inactivity, causing slow cold starts (15-30 seconds). Upgrade to Basic tier recommended for production use to eliminate spindown delays.

%testing
\section{Testing}

The Nala project implements automated testing for the backend API and database layers, along with a continuous integration and continuous deployment (CI/CD) pipeline that runs tests, linters, and formatters on every commit and pull request. This ensures code quality, catches regressions early, and maintains consistent code style across the codebase.

\subsection{Test Files Overview}

All backend tests are located in the \texttt{backend/tests/} directory and use the pytest framework. The project includes both unit tests and integration tests to ensure individual components and full workflows function correctly.

\subsubsection{Backend Tests}

\begin{itemize}
    \item \textbf{test\_api\_endpoints.py} - Tests all FastAPI REST endpoints including:
    \begin{itemize}
        \item Root endpoint (\texttt{/}) and health check endpoints
        \item Chat message endpoint (\texttt{POST /api/v1/chat/message})
        \item Conversation retrieval endpoints
        \item Full conversation flow testing (multi-turn interactions)
        \item Request validation and error handling
    \end{itemize}

    \item \textbf{test\_database.py} - Comprehensive database service layer tests including:
    \begin{itemize}
        \item Conversation CRUD operations (create, read, update, delete)
        \item Message CRUD operations
        \item Conversation history persistence
        \item Message count tracking and accuracy
        \item Cascade delete behavior (deleting conversations removes messages)
        \item Database health checks and connection validation
        \item Serialization methods for API responses
    \end{itemize}

    \item \textbf{conftest.py} - Pytest configuration and shared fixtures:
    \begin{itemize}
        \item Database initialization for testing (uses SQLite for tests)
        \item FastAPI TestClient fixture for making HTTP requests
        \item Mock AI service to avoid requiring Claude/GPT API calls during tests
        \item Sample request data fixtures
    \end{itemize}
\end{itemize}

\subsubsection{AI-Backend Tests}

The AI-backend folder contains connection and persistence tests for the transcript database and conversation history storage:

\begin{itemize}
    \item \textbf{test\_connection.py} - Tests connection to the transcript database on Render. Important for monitoring database connectivity issues, especially when USFConnect blocks port 5432.

    \item \textbf{test\_conversationdb.py} - Tests connection to the conversation history database on Render.

    \item \textbf{test\_session\_persistence.py} - Tests saving and loading session data (user profiles, goals, chat history) across sessions to ensure data persistence works correctly.
\end{itemize}

\subsection{Running Tests Locally}

All tests should be run from the \texttt{backend/} directory. Ensure dependencies are installed first:

\begin{verbatim}
cd backend
pip install -r requirements.txt
\end{verbatim}

\subsubsection{Running All Tests}

To run the complete test suite with coverage reporting:

\begin{verbatim}
pytest -v --tb=short --cov=. --cov-report=term-missing
\end{verbatim}

\subsubsection{Running Specific Test Files}

To run only API endpoint tests:

\begin{verbatim}
pytest tests/test_api_endpoints.py -v --tb=short
\end{verbatim}

To run only database tests:

\begin{verbatim}
pytest tests/test_database.py -v --tb=short
\end{verbatim}

\subsubsection{Running Individual Tests}

To run a specific test function:

\begin{verbatim}
pytest tests/test_api_endpoints.py::test_health_check_endpoint -v
\end{verbatim}

\subsubsection{Test Markers}

Tests are organized using pytest markers defined in \texttt{pytest.ini}:

\begin{itemize}
    \item \texttt{@pytest.mark.api} - API endpoint tests
    \item \texttt{@pytest.mark.unit} - Unit tests
    \item \texttt{@pytest.mark.integration} - Integration tests
    \item \texttt{@pytest.mark.database} - Database tests
\end{itemize}

To run only tests with a specific marker:

\begin{verbatim}
pytest -m database -v
\end{verbatim}

\subsection{CI/CD Pipeline}

The project uses GitHub Actions for continuous integration and deployment. The pipeline is defined in \texttt{.github/workflows/ci.yml} and runs automatically on:

\begin{itemize}
    \item Every push to \texttt{main}, \texttt{develop}, or \texttt{taiyo-backend-skeleton} branches
    \item Every pull request targeting \texttt{main} or \texttt{develop} branches
\end{itemize}

\subsubsection{Pipeline Jobs}

The CI/CD pipeline runs on Ubuntu and performs the following steps in sequence:

\textbf{1. Environment Setup}
\begin{itemize}
    \item Checks out the repository code
    \item Sets up Python 3.11
    \item Caches pip dependencies to speed up builds
    \item Installs all dependencies from \texttt{backend/requirements.txt}
\end{itemize}

\textbf{2. Code Quality Checks}

\textit{Linting with Flake8:}
\begin{verbatim}
flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
flake8 . --count --exit-zero --max-complexity=10
         --max-line-length=127 --statistics
\end{verbatim}

The first flake8 command checks for critical syntax errors and undefined names. The second command reports on code complexity and style issues but does not fail the build (\texttt{--exit-zero}).

\textit{Format Checking with Black and isort:}
\begin{verbatim}
black --check .
isort --check-only .
\end{verbatim}

These commands verify that all Python code follows consistent formatting standards. Black enforces opinionated formatting rules, while isort ensures imports are properly organized. The \texttt{--check} and \texttt{--check-only} flags mean these commands will fail the build if formatting issues are found without automatically fixing them.

\textbf{3. Test Execution}

The pipeline runs three separate test commands:

\begin{verbatim}
pytest tests/test_database.py -v --tb=short
pytest tests/test_api_endpoints.py -v --tb=short
pytest -v --tb=short --cov=. --cov-report=term-missing
\end{verbatim}

The first two commands run specific test files to catch failures early. The final command runs the full test suite with code coverage reporting, showing which lines of code are not covered by tests.

\subsubsection{Pipeline Output}

When the pipeline runs, GitHub Actions provides:
\begin{itemize}
    \item Real-time logs for each step
    \item Red/green status indicators for each job
    \item Detailed error messages if any step fails
    \item Code coverage percentages and missing line numbers
    \item Flake8 statistics on code quality metrics
\end{itemize}

All pull requests must pass the CI/CD pipeline before they can be merged. This ensures that all code merged into the main branches is properly tested, formatted, and free of syntax errors.

\subsubsection{Running CI/CD Steps Locally}

To verify your code will pass CI/CD before pushing, run these commands locally:

\begin{verbatim}
cd backend

# Run linter
python3 -m flake8 . --count --select=E9,F63,F7,F82
                    --show-source --statistics

# Check formatting
python3 -m black --check .
python3 -m isort --check-only .

# Run tests
python3 -m pytest tests/test_api_endpoints.py -v --tb=short
python3 -m pytest -v --tb=short
\end{verbatim}

To automatically fix formatting issues:

\begin{verbatim}
python3 -m black .
python3 -m isort .
\end{verbatim}

\subsection{Testing Best Practices}

When adding new features or fixing bugs, follow these testing guidelines:

\begin{itemize}
    \item Write tests for all new endpoints and database operations
    \item Run the full test suite locally before pushing code
    \item Ensure code coverage does not decrease with new changes
    \item Use mocks to avoid requiring external services (Claude API, Render databases) during tests
    \item Add descriptive docstrings to test functions explaining what is being tested
    \item Run formatters (Black and isort) before committing to avoid CI/CD failures
\end{itemize}

%front end Code
\section{Frontend}

The Nala frontend is a React Native application built using Expo and TypeScript. 
It manages all user-facing features including onboarding, authentication flows, 
chat interactions, text size preferences, and the primary navigation structure. 
This section documents the architecture, file structure, and implementation details 
required for future maintenance and development.

\subsection{Technology Overview}
\begin{itemize}
    \item \textbf{Framework:} React Native + Expo
    \item \textbf{Language:} TypeScript
    \item \textbf{State Management:} React Context API
    \item \textbf{Authentication:} Firebase Authentication
    \item \textbf{API Integration:} FastAPI backend 
    \item \textbf{Navigation:} React Navigation (Native Stack)
\end{itemize}

\subsection{Folder Structure}
The directory structure for the frontend is organized as follows:
\begin{verbatim}
frontend/
  assets/
  src/
    components/
      onboarding/
        BackButton.tsx
        Button.tsx
        Consent.tsx
        DetailsSlide.tsx
        HowItWorksSlide.tsx
        WelcomeSlide.tsx
    config/
      firebaseConfig.ts
    contexts/
        AuthContext.tsx
        TextSizeContext.tsx
    navigation/
        AppNavigator.tsx
        AuthStack.tsx
        MainStack.tsx
    screens/
      ChatOverviewScreen.tsx
      ChatScreen.tsx
      FirebaseTestScreen.tsx
      LoadingScreen.tsx
      LoginScreen.tsx
      OnboardingScreen.tsx
      Settings.tsx
      SignUpScreen.tsx
      WelcomeScreen.tsx
    services/
        ApiService.ts
\end{verbatim}


Each major directory is described below.

\subsection{Assets}
This folder contains images and icons used across the application:
\begin{itemize}
    \item Application icons for Expo (adaptive-icon, favicon, splash)
    \item The official Nala logo (\texttt{nala-logo.png})
\end{itemize}

These assets are referenced in \texttt{app.json} and throughout the UI.

\subsection{Components (Onboarding)}
Located in \texttt{src/components/onboarding}, these files implement reusable UI elements for the onboarding flow:
\begin{itemize}
    \item \textbf{BackButton.tsx} – Consistent back navigation button.
    \item \textbf{Button.tsx} – The Nala-styled next button.
    \item \textbf{Consent.tsx} – Consent information slide.
    \item \textbf{DetailsSlide.tsx} – User information slide.
    \item \textbf{HowItWorksSlide.tsx} – Explains the coaching process.
    \item \textbf{WelcomeSlide.tsx} – Initial welcome screen for onboarding.
\end{itemize}

These slides are used within the main onboarding screen to create an
introduction for new users.

\subsection{Config}
\begin{itemize}
    \item \textbf{firebaseConfig.ts} – Initializes Firebase Authentication.
\end{itemize}

This file must be updated if Firebase credentials or project settings change.
If the Firebase API key, auth domain, project ID, or app ID changes in the Firebase Console, the values in firebaseConfig.ts must be updated.
\subsection{Contexts}
React Context API is used to manage global application state.
    
\subsubsection{AuthContext}
Handles:
\begin{itemize}
    \item User authentication state (logged in / logged out)
    \item Access to Firebase user object
    \item Tracking whether onboarding is completed
\end{itemize}

Navigation relies heavily on \texttt{loggedInUser} and \texttt{hasCompletedOnboarding}.

\subsubsection{TextSizeContext}
Stores UI font size preferences:
\begin{itemize}
    \item Small = 14pt
    \item Medium = 16pt
    \item Large = 20pt
\end{itemize}

Used in Chat, Overview, and Settings screens. Settings updates propagate globally.

\subsection{Navigation}
The navigation system consists of three main stacks:

\begin{itemize}
    \item \textbf{AuthStack} – \texttt{LoginScreen} and \texttt{SignUpScreen}
    \item \textbf{MainStack} – Core app screens (Chat, Overview, Settings)
    \item \textbf{AppNavigator} – Determines which stack to show:
    \begin{itemize}
        \item No user → AuthStack
        \item User but onboarding incomplete → OnboardingScreen
        \item Completed onboarding → MainStack
    \end{itemize}
\end{itemize}

\subsection{Screens}
\begin{itemize}
    \item \textbf{ChatOverviewScreen.tsx} – Entry point to the chat system.
    \item \textbf{ChatScreen.tsx} – Contains the full chat UI:
    \begin{itemize}
        \item Message rendering
        \item Typing animation
        \item Scroll-to-bottom behavior
        \item API request handling
    \end{itemize}
    \item \textbf{FirebaseTestScreen.tsx} – Used during development to confirm Firebase integration.
    \item \textbf{LoadingScreen.tsx} – Shown during initial Firebase auth state check.
    \item \textbf{LoginScreen.tsx} – Email/password login via Firebase.
    \item \textbf{OnboardingScreen.tsx} – Controller for the multi-slide onboarding flow.
    \item \textbf{Settings.tsx} – Text size settings, logout functionality.
    \item \textbf{SignUpScreen.tsx} – Account creation screen.
    \item \textbf{WelcomeScreen.tsx} – Landing screen shown before login.
\end{itemize}

\subsection{Services}
API communication logic is located in \texttt{src/services}. The primary service:

\begin{itemize}
    \item \textbf{ApiService.ts} – is the central layer responsible for API communication. It defines the base URL for the backend using a USE DEPLOYED flag that switches between the Render deployment and the local development server. All API endpoints are constructed from this base URL.
The service includes methods for health checks, sending chat messages, retrieving session data, marking sessions as complete, checking user onboarding status, and fetching progress.
The API calls use native fetch, standardized JSON request bodies, and consistent error handling. Any failed request is logged and either thrown or returned as null depending on the use case
\end{itemize}


\subsection{Authentication Flow}
The frontend handles all Firebase authentication:
\begin{enumerate}
    \item User logs in or signs up via email/password.
    \item Firebase returns a user object.
    \item AuthContext stores the user.
    \item AppNavigator switches to Onboarding or MainStack based on completion.
    \item Logout calls Firebase's \texttt{signOut()} and resets context state.
\end{enumerate}

\subsection{Onboarding Logic}

Before a user can access the main features of the Nala application, they must complete a short onboarding sequence. The onboarding flow ensures that new users understand the purpose of the app, provide required consent, and are ready to begin the coaching program.

The onboarding system relies on a simple state variable stored in \texttt{AuthContext}:

\begin{itemize}
    \item \texttt{hasCompletedOnboarding} – a boolean that indicates whether the user has finished the onboarding slides.
\end{itemize}

This value is retrieved from the backend using the \texttt{/user/status} endpoint and stored alongside the authenticated Firebase user object.

\subsubsection*{Frontend Logic}
The onboarding flow is managed primarily by \texttt{AppNavigator}. The navigator chooses which stack to display based on two conditions:

\begin{enumerate}
    \item If there is \textbf{no authenticated user}, the app shows the \textbf{AuthStack} (Login / Sign Up).
    \item If a user is authenticated but \textbf{onboarding is not completed}, the app displays the \textbf{OnboardingScreen}.
    \item If onboarding is completed, the user is directed to the \textbf{MainStack}, which contains Chat, Overview, and Settings.
\end{enumerate}

\subsubsection*{Onboarding Flow}
The onboarding process is implemented as a series of slide components:

\begin{itemize}
    \item \textbf{Welcome} – Introduces the Nala coaching program.
    \item \textbf{Consent} – Informs the user of data use and requires agreement.
    \item \textbf{How It Works} – Explains the structure of the coaching sessions.
    \item \textbf{Details} – Collects basic user information if needed.
\end{itemize}

Each slide uses shared UI components for consistency. Navigation between slides is controlled inside the \texttt{OnboardingScreen}.

\subsubsection*{Completing Onboarding}
When the user reaches the final slide and selects “Continue,” the following steps occur:

\begin{enumerate}
    \item The frontend calls the \texttt{/user/onboarding} endpoint to save completion state in the backend.
    \item \texttt{AuthContext} updates \texttt{hasCompletedOnboarding = true}.
    \item \texttt{AppNavigator} re-renders and automatically transitions the user into the \textbf{MainStack}.
\end{enumerate}

This onboarding flag is persistent, meaning users only complete onboarding once unless their account data is reset.


\subsection{Chat System}
The ChatScreen implements:
\begin{itemize}
    \item A message object with fields: \texttt{id, sender, text, timestamp}
    \item Dynamic rendering for user vs. Nala messages
    \item Animated typing indicator using React Native's \texttt{Animated} API
    \item Smooth auto-scrolling behavior
    \item Handling of the first message: Nala sends the first message after chat loads
    \item API calls to retrieve Nala's responses via \texttt{ApiService}
\end{itemize}

\subsection{Text Size System}
User preferences are stored globally:
\begin{itemize}
    \item Updated in the Settings screen
    \item Applied across Chat, Overview, and other text-heavy components
\end{itemize}

This improves accessibility and user comfort.

\subsection{Session Unlock Logic}

The session unlock system in the frontend is driven entirely by data returned from 
the backend's \texttt{/session/progress} endpoint. Each progress record contains two 
timestamps:

\begin{itemize}
    \item \texttt{completed\_at} – when a session was finished
    \item \texttt{unlocked\_at} – when the next session becomes available
\end{itemize}

The frontend does not calculate unlock timing itself; instead, it checks the values 
returned by the backend and uses simple comparison logic to determine the state of 
each session card:

\begin{itemize}
    \item Session 1 is always unlocked.
    \item A session is considered unlocked if:
    \begin{itemize}
        \item its \texttt{unlocked\_at} timestamp exists \textbf{and} is in the past, or
        \item the previous session has a \texttt{completed\_at} timestamp.
    \end{itemize}
    \item A session is considered completed if it has a \texttt{completed\_at} value.
    \item Locked sessions display a neutral card style and are non-interactive.
\end{itemize}


\subsection{API Integration}
The frontend communicates with the FastAPI backend via REST endpoints defined in \texttt{ApiService.ts}. The service class handles all HTTP communication and provides methods for chat, session management, and user progress tracking.

\subsubsection{Base Configuration}
The API service supports both local development and production deployment:

\begin{itemize}
    \item \textbf{Production URL:} \url{https://nala-backend-serv.onrender.com}
    \item \textbf{Local URL:} \texttt{http://127.0.0.1:8000}
    \item \textbf{API Version:} \texttt{/api/v1}
\end{itemize}

Toggle between environments by setting \texttt{USE\_DEPLOYED} in \texttt{ApiService.ts:4}.

\subsubsection{Core API Endpoints}

\textbf{Health Check}
\begin{itemize}
    \item \texttt{GET /api/v1/health} - Verify backend connectivity and database status
\end{itemize}

\textbf{Chat Endpoints}
\begin{itemize}
    \item \texttt{POST /api/v1/chat/message} - Send user message and receive AI response
    \begin{itemize}
        \item Request body: \texttt{\{message, user\_id, session\_number, conversation\_id\}}
        \item Response: \texttt{\{response, conversation\_id, message\_id, timestamp, metadata, session\_complete\}}
    \end{itemize}
    \item \texttt{GET /api/v1/chat/conversation/\{id\}} - Retrieve conversation history
    \item \texttt{GET /api/v1/chat/conversations?user\_id=\{uid\}} - List all user conversations
\end{itemize}

\textbf{Session Management}
\begin{itemize}
    \item \texttt{GET /api/v1/session/data/\{user\_id\}/\{session\_number\}} - Load session data (goals, user profile, chat history)
    \item \texttt{GET /api/v1/session/latest/\{user\_id\}} - Get most recent session for user
    \item \texttt{POST /api/v1/session/complete?user\_id=\{uid\}\&session\_number=\{n\}} - Mark session complete and unlock next session
    \item \texttt{GET /api/v1/session/progress/\{user\_id\}} - Retrieve all session progress records
\end{itemize}

\textbf{User Endpoints}
\begin{itemize}
    \item \texttt{GET /api/v1/user/status/\{user\_id\}} - Get user status and onboarding state
    \item \texttt{POST /api/v1/user/onboarding} - Mark user onboarding as complete
\end{itemize}

\subsubsection{Error Handling}
The \texttt{ApiService} class includes error handling with console logging. Failed requests throw errors that are caught by React components and displayed to users via alerts or error states.


\subsection{Common Issues and Fixes}
\begin{itemize}
    \item \textbf{Expo stuck on splash:} Restart Expo or run \texttt{npx expo start -c}
    \item \textbf{Environment variables not updating:} Restart Expo completely
    \item \textbf{Metro bundler errors:} Delete \texttt{node\_modules} and reinstall
    \item \textbf{Backend 503 errors:} Usually caused by Render downtime
\end{itemize}


%current user testing
\section{Current User Testing}
User testing has been done by the team creating this project with some user testing from friends in beta stages. Sponsors have seen live demos and have made comments at this time and those comments have been resolved. Some user testing will be conducted at 2025 CS Night and these comments will be filed for use later. Larger scale user testing has not been conducted and will need to be completed at a later time. 

%things we would like to do in the future
\subsection*{User Testing Summary \& Future Improvements}

User testing to date has primarily been conducted by the project team, along with informal feedback from friends during early beta stages. Our sponsors have also reviewed live demos and provided suggestions, all of which have been addressed in the current build. Additional user testing will take place during \textbf{2025 CS Night}, where feedback will be collected, organized, and used to guide future iterations.

A larger-scale, structured usability study has not yet been completed and will need to be conducted in a later development phase.

\subsubsection*{Planned Enhancements}

Several features are planned to strengthen usability, accessibility, and overall user experience:


\subsection{AI Backend}
\begin{itemize}
\item Continue breaking down the session files (session\_.py and session\_\_manager.py) into smaller utils for reusability and shorter files

\item Unify a lot of the same mechanics (like how the storage util is called in all the files) into the same file and call that instead

\item Implement turn limits into each of the states to prevent getting stuck 

\item Add greater constraint checking 
\begin{itemize}
    \item Checking that Claude doesn't respond with no question when it is not end of session

    \item Checking that Claude doesn't give misinformation or act badly when a user tries to make it spiral

    \item Checking if users need more help than the chatbot can provide 
    (Put user prompt through some checking)
\end{itemize}

\item Implement changing the main AI to being one from OpenAI when Anthropic is down and retry logic fails after given amount of retries

\item Change keyword matching (heuristic) to using Claude to check if something is negative or positive or whether it answers a question or not

\item Refining the smart goal refinement states

\item Reexammine transcripts for correct formatting and conversation pairs

\end{itemize}

\subsection{Backend}

\begin{itemize}
    \item \textbf{Complete pub-sub event system} - Finish implementing the event bus architecture currently in \texttt{archive/pubsub/}. This would enable async, non-blocking database operations for improved performance:
    \begin{itemize}
        \item Event types already defined: MESSAGE\_RECEIVED, RESPONSE\_GENERATED, MESSAGE\_SAVED, CONVERSATION\_CREATED
        \item Would decouple API response from database write operations
        \item Enable better error handling and retry logic for database failures
        \item Support future features like real-time notifications and analytics
    \end{itemize}

    \item \textbf{WebSocket support for real-time streaming} - Implement WebSocket endpoints to enable streaming responses from Claude/GPT:
    \begin{itemize}
        \item Stream tokens as they're generated instead of waiting for complete response
        \item Improve perceived response time and user experience
        \item Reduce frontend timeout issues on long responses
    \end{itemize}

    \item \textbf{Caching layer for RAG retrieval} - Add Redis or in-memory cache for frequently accessed coaching conversation pairs:
    \begin{itemize}
        \item Cache vector search results to reduce database load
        \item Cache common session data to speed up session initialization
        \item Implement TTL-based invalidation strategy
    \end{itemize}

    \item \textbf{Enhanced monitoring and logging} - Implement structured logging and observability:
    \begin{itemize}
        \item Add request/response logging with correlation IDs
        \item Track AI model usage, costs, and latency metrics
        \item Monitor database connection pool health
        \item Alert on error rate thresholds
    \end{itemize}

    \item \textbf{API rate limiting and authentication} - Add production-ready security:
    \begin{itemize}
        \item Implement rate limiting per user/IP to prevent abuse
        \item Add API key authentication for mobile app
        \item Firebase token verification for all endpoints
        \item Request throttling during high load
    \end{itemize}

    \item \textbf{Automated backup and recovery} - Implement database backup strategy:
    \begin{itemize}
        \item Automated daily backups of conversation and session data
        \item Point-in-time recovery capability
    \end{itemize}

    \item \textbf{Recycling persistent data} - Retrain the Nala model using the Conversation History Database
    \begin{itemize}
        \item Train the model on new dynamic data
        \item Export user data for analysis
        \item Updating and revamping the model for continuous improvement
    \end{itemize}
\end{itemize}

\subsection{Frontend}

\begin{itemize}
    \item Goal Overview Page for quick access to progress and upcoming sessions.
    \item Motivational notifications to improve user consistency and engagement.
    \item Google / Apple Sign-In for smoother and more familiar authentication.
    \item Improved chat history interface, animations, microinteractions, and dark mode support.
\end{itemize}


\subsection{General}
\begin{itemize}
    \item Upgrading chat history database to basic tier
    \item For scaling purposes migrate databases and backend to another hosting service
\end{itemize}


\end{document}
